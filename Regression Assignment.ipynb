{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3f3423-5bd1-489c-acaa-2e61534f0978",
   "metadata": {},
   "source": [
    "# Regression \n",
    "\n",
    "# Assignment \n",
    "\n",
    "# 1. What is Simple Linear Regression?\n",
    "\n",
    "Simple Linear Regression is a statistical method used to model the relationship between two variables:\n",
    "\n",
    "one independent variable (X)\n",
    "\n",
    "one dependent variable (Y)\n",
    "\n",
    "It finds the best-fit straight line (Y = a + bX) that predicts Y from X.\n",
    "\n",
    "Example: Predicting someone's weight (Y) based on their height (X).\n",
    "\n",
    "# 2. What are the key assumptions of Simple Linear Regression?\n",
    "\n",
    "The key assumptions of Simple Linear Regression are:\n",
    "\n",
    "1. Linearity: The relationship between X and Y is linear.\n",
    "\n",
    "2. Independence: Observations are independent of each other.\n",
    "\n",
    "3. Homoscedasticity: Constant variance of residuals (errors) across all levels of X.\n",
    "\n",
    "4. Normality of Errors: Residuals (errors) are normally distributed.\n",
    "\n",
    "5. No Multicollinearity: (Only applies to multiple regression; not needed for simple regression as there's only one X).\n",
    "\n",
    "These ensure the model's predictions and interpretations are reliable.\n",
    "\n",
    "# 3. What does the coefficient m represent in the equation Y=mX+c?\n",
    "\n",
    "In the equation Y = mX + c, the coefficient m represents the slope of the line.\n",
    "\n",
    "Meaning:\n",
    " - It shows the change in Y for a one-unit increase in X.\n",
    "\n",
    " - If m > 0, Y increases with X (positive relationship).\n",
    "\n",
    " - If m < 0, Y decreases with X (negative relationship).\n",
    "\n",
    "Example:\n",
    "\n",
    "If m = 2, then for every 1 unit increase in X, Y increases by 2 units.\n",
    "\n",
    "# 4. What does the intercept c represent in the equation Y=mX+c ?\n",
    "\n",
    "In the equation Y = mX + c, the intercept c represents the value of Y when X = 0.\n",
    "\n",
    "Meaning:\n",
    "\n",
    " - It is the point where the line crosses the Y-axis.\n",
    "\n",
    " - It shows the starting value of Y before any change in X.\n",
    "\n",
    "Example:\n",
    "\n",
    "If c = 5, then when X = 0, Y = 5.\n",
    "\n",
    "# 5. How do we calculate the slope m in Simple Linear Regression ?\n",
    "\n",
    "In Simple Linear Regression, the slope 𝑚 of the line Y=mX+c is calculated using the least squares method, which minimizes the sum of squared differences between actual and predicted values.\n",
    "\n",
    "We want to fit the line:\n",
    "\n",
    "Y = mX + c\n",
    "\n",
    "To find the slope m, follow these easy steps:\n",
    "\n",
    "1. Calculate the mean (average) of X and Y:\n",
    "\n",
    " - mean_x = average of X values\n",
    "\n",
    " - mean_y = average of Y values\n",
    "\n",
    "2. Calculate the numerator:\n",
    "\n",
    " - For each pair (x, y), do: (x - mean_x) × (y - mean_y)\n",
    "\n",
    " - Then add all these results together\n",
    "\n",
    "3. Calculate the denominator:\n",
    "\n",
    " - For each x, do: (x - mean_x)²\n",
    "\n",
    " - Then add all these values together\n",
    "\n",
    "4. Finally, divide:\n",
    "\n",
    " - slope m = numerator / denominator\n",
    "\n",
    "This slope m tells us how much Y will increase or decrease when X increases by 1.\n",
    "\n",
    "# 6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
    "\n",
    "**Purpose of Least Squares Method:**\n",
    "\n",
    "- To find the line of best fit in Simple Linear Regression.\n",
    "- It minimizes the total squared difference between actual (Y) and predicted (Ŷ) values.\n",
    "- This makes the model as accurate as possible on the given data.\n",
    "\n",
    "Main Purpose:\n",
    "\n",
    "To minimize the total error between actual and predicted values.\n",
    "\n",
    "More specifically, it minimizes the sum of the squared differences between the actual values (Y) and the predicted values (Ŷ) from the line.\n",
    "\n",
    "   Error = ∑(𝑌−𝑌^)2\n",
    "\n",
    "# 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "\n",
    "R² measures how well the regression line fits the data.\n",
    "It tells us the percentage of variation in Y that is explained by X using the linear model.\n",
    "\n",
    "*Interpretation of R²:\n",
    "\n",
    "R² = 1 → Perfect fit (all points lie exactly on the line)\n",
    "\n",
    "R² = 0 → No fit (line does not explain any variation in Y)\n",
    "\n",
    "Example:\n",
    "\n",
    "If R² = 0.85, it means 85% of the variation in Y is explained by X.\n",
    "\n",
    "R² shows how well the model explains the data:\n",
    "\n",
    "- High R² (close to 1) → Good fit\n",
    "\n",
    "- Low R² (close to 0) → Poor fit\n",
    "\n",
    "\n",
    "# 8. What is Multiple Linear Regression ?\n",
    "\n",
    "Multiple Linear Regression is an extension of simple linear regression.\n",
    "It models the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, X₃, ...).\n",
    "\n",
    " Equation: 𝑌 = 𝑏0 + 𝑏1𝑋1 + 𝑏2𝑋2 + 𝑏3𝑋3 +⋯+ 𝑏𝑛𝑋𝑛 \n",
    "\n",
    "Where:\n",
    "\n",
    " - 𝑌 = dependent variable \n",
    " - 𝑋1,𝑋2,...,𝑋𝑛 = independent variables\n",
    " - 𝑏0 = intercept\n",
    " - 𝑏1,𝑏2,...,𝑏𝑛  = coefficients (slopes for each variable)\n",
    "\n",
    "To understand how multiple factors together affect the outcome (Y), and to predict Y based on multiple inputs.\n",
    "\n",
    "Example:\n",
    "\n",
    "Predicting house price based on:\n",
    "\n",
    "Size of the house (X₁)\n",
    "\n",
    "Number of bedrooms (X₂)\n",
    "\n",
    "Location rating (X₃)\n",
    "\n",
    "\n",
    "# 9. What is the main difference between Simple and Multiple Linear Regression ?\n",
    "\n",
    "| **Feature**                         | **Simple Linear Regression**       | **Multiple Linear Regression**                         |\n",
    "| ----------------------------------- | ---------------------------------- | ------------------------------------------------------ |\n",
    "| Number of Independent Variables (X) | Only 1                             | 2 or more                                              |\n",
    "| Equation                            | Y = mX + c                         | Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ                      |\n",
    "| Use Case                            | Predict Y using one input variable | Predict Y using two or more input variables            |\n",
    "| Example                             | Predict salary based on experience | Predict salary based on experience, education, and age |\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    " - Simple Linear Regression → One X variable\n",
    "\n",
    " - Multiple Linear Regression → Two or more X variables\n",
    "\n",
    "# 10. What are the key assumptions of Multiple Linear Regression ?\n",
    "\n",
    "Key Assumptions of Multiple Linear Regression\n",
    "\n",
    "1. Linearity\n",
    "\n",
    " - The relationship between independent variables and the dependent variable is linear.\n",
    "\n",
    "2. Independence\n",
    "\n",
    " - Observations are independent of each other.\n",
    "\n",
    " - No correlation between residuals (errors).\n",
    "\n",
    "3. Homoscedasticity\n",
    "\n",
    " - Constant variance of errors across all levels of independent variables.\n",
    "\n",
    " - Errors should not spread out or shrink with X.\n",
    "\n",
    "4. Normality of Errors\n",
    "\n",
    " - Residuals (errors) should be normally distributed.\n",
    "\n",
    " - Especially important for hypothesis testing.\n",
    "\n",
    "5. No Multicollinearity\n",
    "\n",
    " - Independent variables should not be highly correlated with each other.\n",
    "\n",
    " - Can be checked using VIF (Variance Inflation Factor).\n",
    "\n",
    "6. No Autocorrelation (mainly for time series data)\n",
    "\n",
    " - Residuals should not be correlated with each other over time.\n",
    "\n",
    "\n",
    "\n",
    "# 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?\n",
    "\n",
    "Heteroscedasticity means the variance of errors is not constant, and this can negatively impact your regression model.\n",
    "\n",
    "Effects on the Model:\n",
    "\n",
    "1. Unreliable Standard Errors\n",
    "\n",
    " - Standard errors may be too small or too large.\n",
    "\n",
    " - This affects the accuracy of confidence intervals and hypothesis tests.\n",
    "\n",
    "2. Incorrect p-values\n",
    "\n",
    " - You might think a variable is significant when it’s not (or vice versa).\n",
    "\n",
    "3. Biased t-tests and F-tests\n",
    "\n",
    " - These tests become unreliable, leading to wrong conclusions.\n",
    "\n",
    "4. Does NOT bias the coefficients\n",
    "\n",
    " - The estimates of regression coefficients (b₀, b₁, ...) are still unbiased,\n",
    "but they are not efficient (not the best possible).\n",
    "\n",
    "\n",
    "# 12. How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
    "\n",
    "Multicollinearity means two or more independent variables are highly correlated.\n",
    "This makes the model unstable and hard to interpret.\n",
    "\n",
    "❗ Problems Caused by Multicollinearity:\n",
    "\n",
    " - Large standard errors of coefficients\n",
    "\n",
    " - Unreliable p-values (wrong conclusions about variable importance)\n",
    "\n",
    " - Coefficients may flip sign or become very sensitive to small changes in data\n",
    "\n",
    " How to Fix or Reduce Multicollinearity:\n",
    " \n",
    "1. Check VIF (Variance Inflation Factor)\n",
    "\n",
    " - If VIF > 5 or 10 → high multicollinearity\n",
    "\n",
    "2. Remove one of the correlated variables\n",
    "\n",
    " - Drop the less important or redundant one\n",
    "\n",
    "3. Combine variables\n",
    "\n",
    " - Create a new feature (e.g., average, ratio, etc.)\n",
    "\n",
    "4. Use Regularization\n",
    "\n",
    " - Apply Ridge Regression (L2) or Lasso Regression (L1)\n",
    "\n",
    "5. Use PCA (Principal Component Analysis)\n",
    "\n",
    " - Converts correlated variables into uncorrelated components (advanced technique)\n",
    "\n",
    "Summary:\n",
    "To reduce multicollinearity:\n",
    "→ Check VIF\n",
    "→ Drop or combine variables\n",
    "→ Use Ridge or Lasso Regression\n",
    "\n",
    "# 13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
    "\n",
    "Categorical variables must be converted into numerical form before using them in regression models.\n",
    "\n",
    "Here are the most common techniques:\n",
    "\n",
    " 1. Label Encoding\n",
    "    \n",
    " - Converts each category into a unique number (0, 1, 2, ...).\n",
    "\n",
    " - Used when the categories have ordinal meaning (e.g., low, medium, high).\n",
    "\n",
    " - Not suitable for nominal (unordered) categories.\n",
    "\n",
    "Example:\n",
    "\n",
    "Size: [Small, Medium, Large] → [0, 1, 2]\n",
    "\n",
    " 2. One-Hot Encoding\n",
    "\n",
    " - Creates a new binary (0/1) column for each category.\n",
    "\n",
    " - Best for nominal (unordered) categorical variables.\n",
    "\n",
    "Example:\n",
    "\n",
    "Color: [Red, Blue, Green] →\n",
    "Red   Blue  Green  \n",
    " 1      0     0  \n",
    " 0      1     0  \n",
    " 0      0     1  \n",
    " \n",
    " 3. Dummy Encoding\n",
    "\n",
    " - Like one-hot encoding, but drops one column to avoid multicollinearity (the \"dummy variable trap\").\n",
    "\n",
    "Example:\n",
    "\n",
    "From 3 categories, create 2 columns only.\n",
    "\n",
    " 4. Binary Encoding (Advanced)\n",
    "\n",
    " - Converts categories into binary code, then splits the digits into separate columns.\n",
    "\n",
    " - Useful when the number of categories is large.\n",
    "\n",
    " 5. Frequency or Count Encoding\n",
    "\n",
    " - Replaces categories with their frequency or count in the dataset.\n",
    "\n",
    "📝 Summary:\n",
    "\n",
    "Use One-Hot Encoding for unordered categories.\n",
    "Use Label Encoding for ordered categories.\n",
    "Drop one column to avoid multicollinearity (Dummy Encoding).\n",
    "\n",
    "\n",
    "# 14. What is the role of interaction terms in Multiple Linear Regression ?\n",
    "\n",
    "Interaction terms are used in regression when the effect of one variable on the target (Y) depends on the value of another variable.\n",
    "\n",
    " Why Use Interaction Terms?\n",
    " \n",
    " - To capture combined effects of variables that do not act independently.\n",
    "\n",
    " - To improve model accuracy when variables influence each other.\n",
    "\n",
    " Example:\n",
    " \n",
    "Suppose you're predicting salary based on:\n",
    "\n",
    " - Experience (X₁)\n",
    "\n",
    " - Education Level (X₂)\n",
    "\n",
    "There may be an interaction:\n",
    "\n",
    "More experience helps more if the person is highly educated.\n",
    "\n",
    "So, you add a new variable:\n",
    "\n",
    "X₁ * X₂ (interaction term)\n",
    "\n",
    "The new model becomes:\n",
    "\n",
    "Y = b₀ + b₁X₁ + b₂X₂ + b₃(X₁ * X₂)\n",
    "\n",
    "\n",
    "# 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
    "\n",
    " Interpretation of Intercept in Simple vs. Multiple Linear Regression\n",
    " \n",
    " Simple Linear Regression\n",
    " \n",
    "Equation:\n",
    "\n",
    "Y=mX+c\n",
    "\n",
    "The intercept (c) is the predicted value of Y when X = 0.\n",
    "\n",
    "It shows where the regression line crosses the Y-axis.\n",
    "\n",
    " Example:\n",
    "\n",
    "If X = years of experience and Y = salary,\n",
    "\n",
    "then c = salary when experience is 0.\n",
    "\n",
    "Multiple Linear Regression\n",
    "\n",
    "Equation:\n",
    "\n",
    "Y = b₁X₁ + b₂X₂ + ... + bₙXₙ\n",
    "\n",
    "The intercept (b₀) is the predicted value of Y when all X variables are 0.\n",
    "\n",
    "It is the starting point of the model when none of the predictors contribute.\n",
    "\n",
    "# 16. What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
    "\n",
    "The slope represents the change in the dependent variable (Y) for a 1 unit increase in the independent variable (X), while keeping other variables constant (in multiple regression).\n",
    "\n",
    " In Simple Terms:\n",
    " \n",
    "     If slope = 5 → Y increases by 5 units when X increases by 1 unit.\n",
    "\n",
    " - A positive slope means Y increases as X increases.\n",
    "\n",
    " - A negative slope means Y decreases as X increases.\n",
    "\n",
    " How Slope Affects Predictions:\n",
    "\n",
    " - The slope determines how much the predicted value of Y will change for changes in X.\n",
    "\n",
    " - It directly controls the angle or steepness of the regression line.\n",
    "\n",
    "# 17. How does the intercept in a regression model provide context for the relationship between variables ?\n",
    "\n",
    "What is the Intercept?\n",
    "\n",
    "The intercept is the value of the dependent variable (Y) when all independent variables (X) are zero.\n",
    "\n",
    "In the equation:\n",
    "\n",
    "Y = b₁X₁ + b₂X₂ + ... + bₙXₙ\n",
    "\n",
    "→ b₀ is the intercept.\n",
    "\n",
    " Why is it Important?\n",
    " \n",
    " - It gives the baseline value of Y.\n",
    "\n",
    " - Helps understand the starting point before any predictors (X₁, X₂...) influence the outcome.\n",
    "\n",
    " Example:\n",
    " \n",
    "If you're predicting salary based on experience and education:\n",
    "\n",
    "Y = b₀ + b₁(Experience) + b₂(Education Level)\n",
    "\n",
    " - The intercept b₀ tells the expected salary when experience and education = 0.\n",
    "\n",
    " - While this may not be realistic, it still provides context for the model.\n",
    "\n",
    "# 18. What are the limitations of using R² as a sole measure of model performance ?\n",
    "\n",
    " Limitations of Using R² as the Sole Measure of Model Performance\n",
    "\n",
    " What is R²?\n",
    " \n",
    "R² (R-squared) shows the proportion of variance in the dependent variable (Y) explained by the independent variables (X).\n",
    "\n",
    "Value ranges from 0 to 1.\n",
    "\n",
    " Limitations of R²:\n",
    "\n",
    "1. Doesn’t Tell About Overfitting\n",
    "\n",
    " - A high R² might mean the model fits training data well but may not generalize to new data.\n",
    "\n",
    "2. Doesn’t Indicate Model Accuracy\n",
    "\n",
    " - R² says how well the model explains variance, but not how accurate predictions are.\n",
    "\n",
    " - Two models can have the same R² but very different errors.\n",
    "\n",
    "3. Affected by Adding Variables\n",
    "\n",
    " - R² always increases when you add more predictors — even if they are not useful.\n",
    "\n",
    " - This can give a false sense of improvement.\n",
    "\n",
    "4. Not Useful for Non-Linear Models\n",
    "\n",
    " - R² assumes a linear relationship. It's not reliable for non-linear models.\n",
    "\n",
    "5.  the Scale of Errors\n",
    "\n",
    " - It doesn't show how big the prediction errors are — metrics like MAE, MSE, RMSE are better for that.\n",
    "\n",
    " Summary:\n",
    "\n",
    "R² is helpful, but should not be used alone.\n",
    "\n",
    "Combine it with other metrics like RMSE, MAE, and adjusted R² for a complete evaluation.\n",
    "\n",
    "\n",
    "# 19. How would you interpret a large standard error for a regression coefficient ?\n",
    "\n",
    "How to Interpret a Large Standard Error for a Regression Coefficient\n",
    "\n",
    " What is Standard Error?\n",
    " \n",
    " - Standard Error (SE) of a coefficient measures the uncertainty or variability in that coefficient’s estimate.\n",
    "\n",
    " - It tells us how much the estimated coefficient would vary if we repeated the study many times.\n",
    "\n",
    "❗ Interpretation of a Large Standard Error:\n",
    "\n",
    "1. Low Confidence in the Coefficient\n",
    "\n",
    " - A large SE means the coefficient is not estimated precisely.\n",
    "\n",
    " - The true value might be far from the estimated value.\n",
    "\n",
    "2. Wide Confidence Interval\n",
    "\n",
    " - Higher SE leads to a wider confidence interval, making the result less reliable.\n",
    "\n",
    "3. Insignificant Predictor\n",
    "\n",
    " - A large SE can result in a high p-value, suggesting the variable might not be statistically significant.\n",
    "\n",
    "4. Possible Multicollinearity\n",
    "\n",
    " - One cause of large SEs is multicollinearity, where predictors are highly correlated with each other.\n",
    "\n",
    "📌 Summary:\n",
    "\n",
    "A large standard error means the model is uncertain about the coefficient’s value.\n",
    "\n",
    "→ The predictor may not be significant.\n",
    "\n",
    "→ Investigate multicollinearity or noisy data.\n",
    "\n",
    "# 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
    "\n",
    "How to Identify Heteroscedasticity in Residual Plots\n",
    "\n",
    " What is a Residual Plot?\n",
    " \n",
    " - A plot of residuals (errors) on the Y-axis vs. predicted values or an independent variable on the X-axis.\n",
    "\n",
    " - Used to check assumptions in regression models.\n",
    "\n",
    " How to Spot Heteroscedasticity:\n",
    "\n",
    " - Heteroscedasticity = Non-constant spread of residuals.\n",
    "\n",
    " - In the plot:\n",
    "\n",
    "  -- Residuals form a funnel, cone, or fan shape.\n",
    "\n",
    "  -- Spread increases or decreases with X (not random).\n",
    "\n",
    "Example Patterns:\n",
    "\n",
    "✔ Good (Homoscedastic): Random scatter, equal spread.\n",
    "\n",
    "✖ Bad (Heteroscedastic): Residuals widen or narrow as X increases.\n",
    "\n",
    "❗ Why It's Important to Address:\n",
    "\n",
    "1. Violates Regression Assumptions\n",
    "\n",
    " - Regression assumes equal error variance (homoscedasticity).\n",
    "\n",
    "2. Leads to Unreliable Statistics\n",
    "\n",
    " - Biased standard errors → Incorrect p-values and confidence intervals.\n",
    "\n",
    "3. Affects Model Trustworthiness\n",
    "\n",
    " - Makes interpretation and predictions less reliable.\n",
    "\n",
    " Summary:\n",
    "\n",
    "Heteroscedasticity shows up as a pattern (like a cone) in residual plots.\n",
    "It must be fixed to ensure valid hypothesis tests and reliable predictions.\n",
    "\n",
    "\n",
    "# 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?\n",
    "\n",
    "What Does It Mean if a Multiple Linear Regression Model Has High R² but Low Adjusted R²?\n",
    "\n",
    " R² (R-squared):\n",
    " \n",
    " - Shows how much of the variation in the dependent variable (Y) is explained by the model.\n",
    "\n",
    " - Always increases when you add more variables — even if they are useless.\n",
    "\n",
    " Adjusted R²:\n",
    " \n",
    " - Adjusts R² based on the number of predictors.\n",
    "\n",
    " - Penalizes the model for adding irrelevant variables.\n",
    "\n",
    " - Can decrease if new variables don’t improve the model.\n",
    "\n",
    "❗ Interpretation:\n",
    "\n",
    "If R² is high but adjusted R² is low, it means:\n",
    "\n",
    " - The model includes too many irrelevant variables.\n",
    "\n",
    " - Those variables are not adding real value.\n",
    "\n",
    " - The model may be overfitting the training data.\n",
    "\n",
    " Summary:\n",
    "\n",
    "High R² but low adjusted R² → Your model looks good at first,\n",
    "but many predictors are not useful and may hurt generalization.\n",
    "\n",
    "\n",
    "# 22. Why is it important to scale variables in Multiple Linear Regression ?\n",
    "\n",
    " What is Scaling?\n",
    " \n",
    " - Scaling means adjusting variables so they are on the same scale, usually by:\n",
    "\n",
    " - Standardization (mean = 0, std = 1)\n",
    "\n",
    " - Normalization (values between 0 and 1)\n",
    "\n",
    "❗ Why Scaling is Important:\n",
    "\n",
    "1. Improves Model Interpretability\n",
    "\n",
    " - Makes coefficients easier to compare when variables are on the same scale.\n",
    "\n",
    "2. Helps with Regularization\n",
    "\n",
    " - Techniques like Ridge and Lasso Regression are sensitive to variable scales.\n",
    "\n",
    " - Without scaling, features with larger values dominate the penalty terms.\n",
    "\n",
    "3. Avoids Numerical Instability\n",
    "\n",
    " - Large differences in variable ranges can cause unstable or inaccurate computations in some libraries.\n",
    "\n",
    "4. Improves Convergence Speed\n",
    "\n",
    " - Optimization algorithms (like gradient descent) work better with scaled data.\n",
    "\n",
    "# 23. What is polynomial regression ?\n",
    "\n",
    "Polynomial Regression is a type of regression that models the relationship between the independent variable (X) and the dependent variable (Y) as a polynomial of degree n.\n",
    "\n",
    " Equation:\n",
    "\n",
    " Y = b₀ + b₁X + b₂X²\n",
    " \n",
    " - Unlike linear regression (which fits a straight line), polynomial regression can fit curved lines.\n",
    "\n",
    " When to Use:\n",
    "\n",
    " - When the relationship between X and Y is non-linear.\n",
    "\n",
    " - When a linear model underfits the data.\n",
    "\n",
    " Example:\n",
    "\n",
    "Suppose you're predicting growth based on age:\n",
    "\n",
    "Linear regression:     Y = b₀ + b₁X        → straight line\n",
    "Polynomial regression: Y = b₀ + b₁X + b₂X² → curve\n",
    "\n",
    "\n",
    "# 24. How does polynomial regression differ from linear regression ?\n",
    "\n",
    "| Feature                  | **Linear Regression**                   | **Polynomial Regression**                               |\n",
    "| ------------------------ | --------------------------------------- | ------------------------------------------------------- |\n",
    "| **Model Equation**       | Y = b₀ + b₁X                            | Y = b₀ + b₁X + b₂X² + b₃X³ + ... + bₙXⁿ                 |\n",
    "| **Type of Relationship** | Assumes a **linear** relationship       | Captures **non-linear** relationships                   |\n",
    "| **Line Shape**           | Straight line                           | Curved line (parabola, cubic curve, etc.)               |\n",
    "| **Features Used**        | Uses only the original X                | Uses powers of X (X, X², X³, ...)                       |\n",
    "| **Complexity**           | Simple and easy to interpret            | More complex, risk of overfitting if degree is too high |\n",
    "| **When to Use**          | When data follows a straight-line trend | When data shows a curved or non-linear pattern          |\n",
    "\n",
    "\n",
    "# 25. When is polynomial regression used ?\n",
    "\n",
    "Polynomial regression is used when the relationship between the independent variable (X) and dependent variable (Y) is non-linear.\n",
    "\n",
    " Use Polynomial Regression When:\n",
    "\n",
    "1. Data shows a curved pattern\n",
    "\n",
    " - The data points don’t follow a straight line.\n",
    "\n",
    " - A linear model underfits the data.\n",
    "\n",
    "2. You want to model complex relationships\n",
    "\n",
    " - Real-world scenarios where effects change over time, like growth rates or acceleration.\n",
    "\n",
    "3. The residual plot of linear regression shows a pattern\n",
    "\n",
    " - If residuals (errors) are not randomly scattered → linear model is not enough.\n",
    "\n",
    " Example Use Cases:\n",
    " \n",
    " - Predicting housing prices (where prices don’t increase linearly with size)\n",
    "\n",
    " - Modeling population growth, chemical reactions, or physics experiments\n",
    "\n",
    " - Estimating profits over time when the trend isn't straight\n",
    "\n",
    " Tip:\n",
    "\n",
    "Use the lowest degree polynomial that gives a good fit,\n",
    "\n",
    "because higher degrees can overfit and reduce model generalization.\n",
    "\n",
    "# 26. What is the general equation for polynomial regression ?\n",
    "\n",
    "General Equation for Polynomial Regression\n",
    "\n",
    "The general equation of a Polynomial Regression model is:\n",
    "\n",
    "Y = b₀ + b₁X + b₂X² + b₃X³ + ... + bₙXⁿ \n",
    "\n",
    "Where:\n",
    "\n",
    " - 𝑌 = Predicted (dependent) variable\n",
    "\n",
    " - 𝑋 = Independent variable\n",
    "\n",
    " - b₀ = Intercept\n",
    "\n",
    " - b₁ , b₂, ..., + bₙ = Coefficients for each degree of X\n",
    "\n",
    " - 𝑛 = Degree of the polynomial\n",
    "\n",
    " Example (Degree 3):\n",
    "\n",
    " Y = b₀ + b₁X + b₂X² + b₃X³ \n",
    " \n",
    "This allows the model to fit non-linear curves in the data.\n",
    "\n",
    "\n",
    "# 27. Can polynomial regression be applied to multiple variables ?\n",
    "\n",
    "Yes, polynomial regression can be extended to handle multiple independent variables (also called multivariate polynomial regression).\n",
    "\n",
    "\n",
    "🧮 General Equation (2 variables, degree 2):\n",
    "\n",
    "Y = b₀ + b₁·X₁ + b₂·X₂ + b₃·X₁² + b₄·X₂² + b₅·X₁·X₂\n",
    "\n",
    "This Includes:\n",
    "\n",
    "Linear terms:\n",
    "\n",
    "X₁, X₂\n",
    "\n",
    "Quadratic terms:\n",
    "\n",
    "X₁², X₂²\n",
    "\n",
    "Interaction term:\n",
    "\n",
    "X₁·X₂\n",
    "\n",
    "\n",
    "# 28. What are the limitations of polynomial regression ?\n",
    "\n",
    "1. Overfitting\n",
    "\n",
    ". Higher-degree polynomials can fit the training data too closely.\n",
    "\n",
    "This reduces the model's ability to generalize to new/unseen data.\n",
    "\n",
    "2. Increased Complexity\n",
    " \n",
    "Adding more polynomial terms increases the number of features quickly.\n",
    "\n",
    "This can make the model harder to interpret and slower to train.\n",
    "\n",
    "3. Extrapolation Issues\n",
    " \n",
    "Polynomial curves can behave unpredictably outside the range of the training data.\n",
    "\n",
    "Leads to poor predictions for extreme input values.\n",
    "\n",
    "4. Sensitive to Outliers\n",
    "\n",
    "Polynomial models can be highly affected by outliers, which can distort the curve significantly.\n",
    "\n",
    " 5. Multicollinearity\n",
    "\n",
    "Polynomial features (like X, X², X³) are often correlated.\n",
    "\n",
    "This can lead to unstable coefficient estimates.\n",
    "\n",
    "# 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
    "\n",
    "Methods to Evaluate Model Fit When Selecting Polynomial Degree\n",
    "Choosing the right degree for a polynomial model is crucial to avoid underfitting or overfitting. Here are some common methods:\n",
    "\n",
    " 1. Train-Test Split\n",
    "    \n",
    " - Split your data into training and testing sets.\n",
    "\n",
    " - Train the model on training data with different degrees.\n",
    "\n",
    " - Choose the degree with the lowest test error (e.g., MSE or RMSE).\n",
    "\n",
    " 2. Cross-Validation\n",
    "\n",
    " - Use k-fold cross-validation to test the model's performance on multiple data splits.\n",
    "\n",
    " - Helps ensure the chosen degree generalizes well to unseen data.\n",
    "\n",
    " 3. Plotting Error Curves\n",
    "\n",
    " - Plot training and testing error vs. polynomial degree.\n",
    "\n",
    " - Ideal degree is where the testing error is lowest and does not increase afterward.\n",
    "\n",
    " 4. R² and Adjusted R²\n",
    "\n",
    " - R² increases with degree, but Adjusted R² penalizes complexity.\n",
    "\n",
    " - Choose the degree with highest Adjusted R².\n",
    "\n",
    " 5. AIC / BIC (Advanced)\n",
    "\n",
    " - Akaike Information Criterion and Bayesian Information Criterion are metrics that balance model fit and complexity.\n",
    "\n",
    " - Lower AIC/BIC values are better.\n",
    "\n",
    "\n",
    "\n",
    "# 30. Why is visualization important in polynomial regression ?\n",
    "\n",
    "1. Understand the Shape of the Fit\n",
    "\n",
    " - Polynomial regression models curves, not straight lines.\n",
    "\n",
    " - Visualization helps you see how well the curve fits the data.\n",
    "\n",
    "2. Detect Overfitting or Underfitting\n",
    "\n",
    " - If the curve is too wiggly → Overfitting\n",
    "\n",
    " - If the curve is too simple → Underfitting\n",
    "\n",
    " - A plot helps you visually choose a reasonable degree for the polynomial.\n",
    "\n",
    "3. Reveal Outliers and Patterns\n",
    "\n",
    "- Visualization helps spot outliers, non-random patterns, or trends the model may not capture well.\n",
    "\n",
    "4. Compare Models\n",
    " \n",
    " - You can visually compare how different polynomial degrees perform on the same data.\n",
    "\n",
    "# 31. How is polynomial regression implemented in Python ?\n",
    "\n",
    "We can implement polynomial regression using PolynomialFeatures and LinearRegression from sklearn.\n",
    "\n",
    "Step-by-Step Code Example\n",
    "\n",
    "1. Import Libraries\n",
    "\n",
    "       import numpy as np\n",
    "       import matplotlib.pyplot as plt\n",
    "       from sklearn.linear_model import LinearRegression\n",
    "       from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "2. Create Sample Data\n",
    "\n",
    "     # Generate sample data\n",
    "       X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n",
    "       y = np.array([3, 6, 10, 15, 21, 28, 36, 45, 55])  # roughly y = X^2\n",
    "\n",
    " \n",
    "3. Transform Features to Polynomial (e.g., degree 2)\n",
    "\n",
    "       poly = PolynomialFeatures(degree=2)\n",
    "       X_poly = poly.fit_transform(X)\n",
    "\n",
    "\n",
    "4. Fit the Model\n",
    "\n",
    "       model = LinearRegression()\n",
    "       model.fit(X_poly, y)\n",
    "\n",
    "\n",
    "5. Make Predictions\n",
    "\n",
    "       y_pred = model.predict(X_poly)\n",
    "\n",
    "\n",
    "6. Visualize the Results\n",
    "\n",
    "       plt.scatter(X, y, color='blue', label='Actual')\n",
    "       plt.plot(X, y_pred, color='red', label='Polynomial Fit')\n",
    "       plt.xlabel(\"X\")\n",
    "       plt.ylabel(\"y\")\n",
    "       plt.title(\"Polynomial Regression (Degree = 2)\")\n",
    "       plt.legend()\n",
    "       plt.grid(True)\n",
    "       plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd2a3f-05d3-4bef-b6fc-aae501b2fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
